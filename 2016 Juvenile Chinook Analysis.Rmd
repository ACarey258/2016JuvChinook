---
title: "2016 Juvenile Chinook"
output: html_notebook
---
WDFW TBiOS 
2016 Monitoring toxic contaminants in the juvenile Chinook salmon

```{r}
# First step, clear workspace to make sure every thing will work, rm means remove
rm(list=ls(all=TRUE))

# load required packages/libraries
library(readxl)
library(tidyverse)
library(magrittr)
library(forcats)
library(RColorBrewer)
library(stringr)
library(Rmisc)
library(modelr)
library(readr)
```
R for Data Science - Chapter 7, Exploratory Data Analysis (EDA)

Two types of questions to ask about your data to help guide your research:
1. What type of variation occurs within my varaibles?
2. What type of covariation occurs between my variables?

VARIATION is he tendency of the values of a variable to change from measurement to measurement. Every variable has its own pattern of variation, which can reveal interesting information. The best way to understand that pattern is to visualize the distribution of the variable's values.

Bar charts are best to examine the distribution (i.e., counts) of categorical variables.

Load in 2016 Juvenile Chinook dataset
```{r}
#set data path
paths = list("C:\\data\\GitHub\\2016JuvChinook\\2016JuvChin_ForR_5.1.19.xlsx",
             "C:\\data\\GitHub\\2016JuvChinook\\Outfiles\\")

#set outfile
outfile = paths[[2]]

#read in data
JuvChin16 <- read_excel(paths[[1]],"2016JuvChin")

#set factors
#JuvChin16$MajorityOrigin <- as.factor(JuvChin16$MajorityOrigin)
#JuvChin16$RiverSystem <- as.factor(JuvChin16$RiverSystem)
#JuvChin16$Habitat <- as.factor(JuvChin16$Habitat)

#spelling error - change Stilliguamish to Stillaguamish
JuvChin16$RiverSystem <- gsub("Stilliguamish", "Stillaguamish", JuvChin16$RiverSystem)


#remove the "< "" from all the Non-detects
#JuvChin16 <- data.frame(lapply(JuvChin16, function(x) {
                        #gsub("< ", "", x)
                        #})) #changes all columns to a factor


#create a new dataframe and select only the majority origin of Hatchery or Wild
ChinOrigin <- filter(JuvChin16, MajorityOrigin != "Hatchery/Wild") #removes Hatchery/Wild comps

#create a new dataframe and select only the delta and freshwater (Lake WA) samples
ChinHab <- filter(ChinOrigin, Habitat == "Delta" | Habitat == "Freshwater")

#creat a dataframe with just the data needed for analysis
SumPOPs <- as.data.frame(ChinHab[c(1,3,5,8:9,13:14,74,81,93:95,98:99)])
#TotalPCBs is labeled as a <dbl>. Is THAT the issue?

#SumPOPs$RiverSystem <- as.factor(SumPOPs$RiverSystem)
#SumPOPs <- as.numeric(SumPOPs$TotalPCBs) #makes the df a Value and doesn't change it to a numeric!Code wrong??

byDelta <- group_by(SumPOPs, RiverSystem, Habitat) #groups data by River System and Habitat 

(summarise(byDelta, AvgTPCBs = mean(TotalPCBs, na.rm = TRUE))) #calculates the mean TPCBs by River and Habitat
```
Using group_by and summarise with a pipe
```{r}
SumPOPs$MajorityOrigin <- as.factor(SumPOPs$MajorityOrigin)
SumPOPs$RiverSystem <- as.factor(SumPOPs$RiverSystem)
SumPOPs$Habitat <- as.factor(SumPOPs$Habitat)

#not working - affects a ggplot down below
sumPCBs <- SumPOPs %>%
  group_by(RiverSystem, Habitat) %>%
  summarise(
    count = n(), #Error: n() should only be called in a data context 
    MnTPCBs = mean(TotalPCBs, na.rm = TRUE), #na.rm = TRUE removes NAs/missing values prior to calculation
    StdDevPCBs = sd(TotalPCBs, na.rm = TRUE)
  )
```

The KEY to asking good follow-up questions after visualizing your data is to rely on your curiosity as well as skepticism:
1. What do you want to learn more about?
2. How could this be misleading?

Looking at both the bar chart and the box plot, you can see that PCBs in juvenile Chinook salmon are highest in fish migrating through the urbanized areas of the Green/Duwamish, Puyallup, Lake Washington and Snohomish.  The concentrations of PCBs decrease with increased distance from the urban areas of Puget Sound.

Based on past work, this is not surprising and it has met our expectations.

COVARIATION
Covariation describes the behavior between variables.  It is the tendency for the values of two or more variables to vary together in a related way.

The best way to spot covariation is to visualize the relationship between two or more variables.  How you do that should depend on the type of variables involved.

One method to do this is to produce a box plot - a visual shorthad for a distribution of values that is popular among statisticians.
  - the Box stretches from the 25th percentile of the distribution to the 75th percentile (the interquartile range; IQR)
  - In the middle of the box is the median line (50th percentile of the distribution)
  - points that fall more than 1.5x the IQR are plotted separately
  - A line/whisker extends from each side of the box and goes to the farthest non-outlier point in the distibution 
  (see Section 7.5.1 for a depiction of the box plot with labels)

Create box plots of PCBs by RiverSystem and Delta
```{r}
#set factors in the order you want them displayed in the figure (bottom to top for a horizontal box plot)
byDelta$RiverSystem <- factor(byDelta$RiverSystem, levels = c("Elwha", "Dungeness", "Duckabush","Skokomish",
                                                              "Nisqually", "Puyallup", "Duwamish", "Lk Washington",
                                                              "Snohomish", "Stillaguamish", "Skagit","Nooksack"))

ggplot(data = byDelta, mapping = aes(x = RiverSystem, y = TotalPCBs, fill = RiverSystem)) +
  geom_boxplot() +
  coord_flip() + #creates a horizontal box plot
  labs(title = "TPCBs measured in whole body (less guts) juvenile Chinook salmon",
       x = "River System",
       y = "Total PCBs (ng/g wet weight in whole body)") +
  theme_bw() +
  theme(legend.position = "none") #removes the legend
```

Create a bar graph using the sumPCBs dataframe that was just calculated
```{r}
sumPCBs$RiverSystem <- factor(sumPCBs$RiverSystem, levels = c("Nooksack", "Skagit", "Stillaguamish", "Snohomish",
                                                              "Lk Washington","Duwamish", "Puyallup","Nisqually",
                                                              "Skokomish", "Duckabush", "Dungeness","Elwha"))

ggplot(data = sumPCBs, mapping = aes(x = RiverSystem, y = MnTPCBs, fill = RiverSystem)) +
  geom_bar(colour = "black", stat = "identity") +
  guides(fill = FALSE) +
  labs(title = "Mean TPCBs (ng/g ww) measured in juvenile Chinook salmon",
       x = "River System",
       y = "Mean TPCBs (ng/g ww)") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank())
```

To add error bars - Calculate standard error and 95% CI using summarySE()
```{r}
(MoreSums <-summarySE(SumPOPs, measurevar = "TotalPCBs", groupvars = c("RiverSystem", "Habitat"))) 
#Some NaNs produced because Nooksack only has one delta sample

#set factors in the order you want them displayed in the bar chart from left to right
MoreSums$RiverSystem <- factor(MoreSums$RiverSystem, levels = c("Nooksack", "Skagit", "Stillaguamish", "Snohomish",
                                                              "Lk Washington","Duwamish", "Puyallup","Nisqually",
                                                              "Skokomish", "Duckabush", "Dungeness","Elwha"))
write_csv(MoreSums, "TPCBsSumStats.csv")
```

Create a bar graph with 95% CI error bars
```{r}
ggplot(MoreSums, mapping = aes(x = RiverSystem, y = TotalPCBs, fill = RiverSystem)) +
  geom_bar(position = position_dodge(), colour = "black", stat = "identity") + #Stat=identity uses values from a dataframe
  guides(fill = FALSE) + #removes legend
  geom_errorbar(aes(ymin = TotalPCBs - ci, ymax = TotalPCBs + ci),
               width = 0.2, #width of error bars
               position = position_dodge(0.9)) +
    labs(title = "Mean TPCBs (ng/g ww) +/- 95% CI measured in juvenile Chinook salmon",
       x = "River System",
       y = "Mean TPCBs (ng/g ww)") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  scale_y_continuous(expand = c(0,0), limits = c(0,70)) # expand removes the floating bar effect above the x-axis
ggsave("TPCBsByRiver.png")

```
The error message "Removed 1 rows containing missing values (geom_errorbar)" refers to the 1 nooksack sample which error bars were unable to be calculated for.

PATTERNS AND MODELS
Patterns in your data provide clues about relationships. When you see a pattern ask yourself:
1. Could this pattern be due to conicidence (i.e., random chance)?
2. How can you describe the relationship implied by the pattern?
3. How strong is the relationship implied by the pattern?
4. What other variables might affect the relationship?
5. Does the relationship change if you look a individual subgroups of the data?

Scatterplot of Mean Composite Length vs. TPCBs
```{r}
ggplot(data = SumPOPs) +
  geom_point(mapping = aes(x = MCL_mm, y = TotalPCBs)) #not much of a pattern
```

Scatterplot of Mean Composite Weight vs TPCBs
```{r}
ggplot(data = SumPOPs) +
  geom_point(mapping = aes(x = MCW_g, y = TotalPCBs)) #no pattern
```

Scatterplot of %Lipids vs TPCBs
```{r}
colnames(SumPOPs)[colnames(SumPOPs) == "Lipids_%"] <- "Lipids" #R doesn't love the "_%"

ggplot(data = SumPOPs) +
  geom_point(mapping = aes(x = Lipids, y = TotalPCBs)) #maybe a little bit of a pattern?
```

MODELS are a tool for extracting patterns out of data. 

Models compute residuals - the difference between the predicted value and the actual value.

Models remove strong relationships so that the underlying subtleties of other relationships can be explored.
```{r}
mod <- lm(log10(TotalPCBs) ~ log10(Lipids), data = SumPOPs)

SumPOPs2 <- SumPOPs %>%
  add_residuals(mod) %>%
  mutate(resid = exp(resid))

ggplot(data = SumPOPs2) +
  geom_point(mapping = aes(x = TotalPCBs, y = resid)) #not sure if I should be graphing TotalPCBs here
ggsave("TotalPCBsResid.pdf")

```

TIBBLES

Coerce a dataframe (byDelta = data for all the juvenile Chinook collected in the deltas and Lake WA in 2016) to a tibble.
```{r}
as_tibble(byDelta)
```

Tibbles NEVER change the type of the inputs (e.g., it never converts strings to factors!), it NEVER changes the names of variables, and it NEVER creates row names.

There are two main differences in the usage of a tibble vs. a classic data.frame: printing and subsetting.

PRINTING A TIBBLE

Tibbles only show a portion of your data so your console doesn't get overwhelmed with large datasets. You can control the displayed number of rows and columns by using print().

```{r}
byDelta %>%
  print(n = 15, width = Inf) #Inf shows all columns
  
```
  
```{r}
byDelta %>%
  view() #opens the tibble in a scrollable view of the complete dataset
```
  
SUBSETTING

If you want to pull out a single variable, you'll need to use the following "tools"
$ - extracts by name
[[ - extracts by name or position

Extract by Name
```{r}
byDelta$TotalPCBs
```
```{r}
byDelta[["TotalPCBs"]]
```
Extract by position
```{r}
byDelta[[1]] #1 refers to the column number
```

When using these tools in a pipe, you need to use the special placeholder .
```{r}
byDelta %>% .$TotalPCBs
```
Or
```{r}
byDelta %>% .[["TotalPCBs"]]
```

NOTE - some older functions don't work with tibbles so use as.data.frame() to turn a tibble back to a data.frame.
(older functions don't work with [ )

DATA IMPORT
```{r}
JuvChin <- read_csv("C:\\data\\GitHub\\2016JuvChinook\\2016JuvChin_ForR_5.1.19.csv")

# prints out a column specification that gives the name and type of each column - important for parsing a file
```

PARSING A VECTOR

These functions take a character vector and return a more specialized vector like a logical, integer or date
SECTION 11.4


```{r}
JuvChin

```

The parse_*() functions are uniform: the first argument is a character vector to parse and the na argument specifies which strings should be treated as missing.

```{r}
parse_integer(c("1", "231", ".", "456"), na = ".")
```

If parsing fails you WILL get a warning.

```{r}
x <- parse_integer(c("123", "345", "abc", "123.45"))
```

If there are many parsing failures, you'll need to use problems() to get the complete set.  This returns a tibble which you can then manipulate with dplyr.

```{r}
problems(x)
```

Using parsers is mostly a matter of understanding what's available and how they deal with different types of input. There are eight particularly important parsers:
1. parse_logical() and parse_integer() - parse logicals and integers
2. parse_double() is a strict numeric parser and parse_number() is a flexible numeric parser. These are more difficult than you might expect because different parts of the world write numbers in different ways.
3. parse_character() - character encodings is important to remember for complications
4. parse_factor() - creates factors
5. parse_datetime(), parse_date(), parse_time() allow you to parse various date and time specifications. These are the most complicated because there are so many ways to write dates.

11.3.1 Numbers

Numbers can be tricky:
1. People write numbers differently in different parts of the world.
2. Numbers are often surounded by other characters that provide some context, like "$1000" or "10%".
3. Numbers often contain "grouping" characters to make them easy to read, like "1,000,000" and these grouping characters vary around the world.

When parsing numbers, the most important option is the character you use for the decimal mark. You can override the default value of "." by creating a new locale and setting the decimal_mark argument.
```{r}
parse_double("1.23")
```

```{r}
parse_double("1,23", locale = locale(decimal_mark = ","))
```

parse_number() addresses the second problem: it ignores non-numeric characters before and after the number.

```{r}
parse_number("<100") #helpful for non-detects
```
```{r}
parse_number("1.21%") #helpful for lipid percentages
```
The final problem is addressed by the combination of the parse_number() and the locale as parse_numer() will ignore the "grouping mark"
```{r}
# Used in America
parse_number("$123,456,789")
#> [1] 1.23e+08

# Used in many parts of Europe
parse_number("123.456.789", locale = locale(grouping_mark = "."))
#> [1] 1.23e+08

# Used in Switzerland
parse_number("123'456'789", locale = locale(grouping_mark = "'"))
#> [1] 1.23e+08
```

11.3.2 Factors

R uses factors to represent categorical variables that have a known set of possible values. Give parse_factor() a vector of known levels to generate a warning whenever an unexpected value is present.
```{r}
fruit <- c("apple", "banana")
parse_factor(c("apple", "banana", "bananana"), levels = fruit)

```

11.3.4 Dates, dates-times, and times

Three different parsers associated with dates and times:
1. date = the number of days since 1970-01-01
2. date-time = the number of seconds since midnight 1970-01-01
3. time = the number of seconds since midnight

parse_datetime() expects an ISO8601 date-time.  
ISO8601 is an international standard in which the componenets of a date are organized from biggest to smallest: year, month, day, hour, minute, second
```{r}
parse_datetime("2010-10-01T2010") #if time is omitted, it will be set to midnight
```

